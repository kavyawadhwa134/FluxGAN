{"metadata":{"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"CGAN","metadata":{},"execution_count":null,"outputs":[],"id":"7271c233-79be-4c2c-9dde-8bb68ae6fbad"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\nimport csv\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import StepLR\n\n# Configuration\ncheckpoint_dir = '/home/jovyan/FluxGAN/plots/checkpoint'\nloss_log_file = '/home/jovyan/FluxGAN/plots/loss_log.csv'\ncheckpoint_interval = 1000\nnum_epochs = 30001\nbatch_size = 512\nnoise_dim = 100\n\n# Setup CUDA device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.backends.cudnn.benchmark = True  # Enable cudnn auto-tuner\n\n# Setup directories\nos.makedirs(checkpoint_dir, exist_ok=True)\nif not os.path.exists(loss_log_file):\n    with open(loss_log_file, 'w') as f:\n        f.write('Epoch,D Loss,G Loss\\n')\n\n# Load and preprocess data\ndata = pd.read_csv('./flux_burnup_dataset.csv')\nX = data[['Enrichment (%)', 'Flux', 'Burnup']].values\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Save dataset stats for inference\ndata_min = scaler.data_min_\ndata_max = scaler.data_max_\n\n# Models for CGAN\nclass Generator(nn.Module):\n    def __init__(self, noise_dim=100):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(noise_dim + 3, 256),\n            nn.LeakyReLU(0.2),\n            nn.LayerNorm(256),\n            nn.Linear(256, 128),\n            nn.LeakyReLU(0.2),\n            nn.LayerNorm(128),\n            nn.Linear(128, 3),\n            nn.Tanh()  # Output activation function for continuous data\n        )\n        self.apply(self.init_weights)\n\n    def forward(self, z, conditions):\n        x = torch.cat([z, conditions], dim=1)\n        return self.net(x)\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(6, 256),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.LayerNorm(256),\n            nn.Linear(256, 128),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.LayerNorm(128),\n            nn.Linear(128, 1)\n        )\n        self.apply(self.init_weights)\n\n    def forward(self, x, conditions):\n        x = torch.cat([x, conditions], dim=1)\n        return self.net(x)\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n\n# Initialize models and move to GPU\ngenerator = Generator(noise_dim).to(device)\ndiscriminator = Discriminator().to(device)\n\n# Optimizers with adjusted learning rates\noptimizer_G = optim.Adam(generator.parameters(), lr=0.002, betas=(0.5, 0.999))  # Increase learning rate for Generator\noptimizer_D = optim.Adam(discriminator.parameters(), lr=0.00005, betas=(0.5, 0.999))  # Decrease learning rate for Discriminator\n\n# Learning Rate Scheduler\nscheduler_G = StepLR(optimizer_G, step_size=1000, gamma=0.5)\nscheduler_D = StepLR(optimizer_D, step_size=1000, gamma=0.5)\n\n# Loss function\nadversarial_loss = nn.BCEWithLogitsLoss()\n\n# Mixed Precision Scaler\nscaler = GradScaler()\n\n# Checkpoint functions\ndef save_checkpoint(epoch):\n    path = os.path.join(checkpoint_dir, f'checkpoint_{epoch}.tar')\n    torch.save({\n        'epoch': epoch,\n        'generator_state_dict': generator.state_dict(),\n        'discriminator_state_dict': discriminator.state_dict(),\n        'optimizer_G_state_dict': optimizer_G.state_dict(),\n        'optimizer_D_state_dict': optimizer_D.state_dict(),\n        'data_min': data_min,\n        'data_max': data_max\n    }, path)\n    print(f\"[Checkpoint] Saved at epoch {epoch}\")\n\ndef load_checkpoint():\n    files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.tar')]\n    if not files:\n        print(\"[Checkpoint] No checkpoint found. Starting fresh.\")\n        return 0\n    \n    latest = max(files, key=lambda f: int(f.split('_')[1].split('.')[0]))\n    path = os.path.join(checkpoint_dir, latest)\n    \n    try:\n        checkpoint = torch.load(path, map_location=device)\n        generator.load_state_dict(checkpoint['generator_state_dict'])\n        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n        optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n        optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n        print(f\"[Checkpoint] Loaded from epoch {checkpoint['epoch']}\")\n        return checkpoint['epoch'] + 1\n    except Exception as e:\n        print(f\"[Checkpoint] Error loading: {str(e)}. Starting fresh.\")\n        return 0\n\n# Load checkpoint if exists\nstart_epoch = load_checkpoint()\n\n# Convert data to CUDA tensors\ndataset = TensorDataset(torch.tensor(X_scaled, dtype=torch.float32))\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n\n# Training loop\nfor epoch in range(start_epoch, num_epochs):\n    for real_data in dataloader:\n        real_data = real_data[0].to(device, non_blocking=True)\n        current_batch_size = real_data.size(0)\n        \n        # Train Discriminator\n        optimizer_D.zero_grad(set_to_none=True)\n        \n        # Real data with label smoothing\n        real_labels = torch.full((current_batch_size, 1), 0.9, device=device)  # Smoothing real labels\n        real_output = discriminator(real_data, real_data)\n        d_loss_real = adversarial_loss(real_output, real_labels)\n        \n        # Fake data with label smoothing\n        z = torch.randn(current_batch_size, noise_dim, device=device)\n        fake_data = generator(z, real_data)\n        fake_labels = torch.full((current_batch_size, 1), 0.1, device=device)  # Smoothing fake labels\n        fake_output = discriminator(fake_data.detach(), real_data)\n        d_loss_fake = adversarial_loss(fake_output, fake_labels)\n        \n        # Total discriminator loss\n        d_loss = (d_loss_real + d_loss_fake) / 2\n        d_loss.backward()\n        optimizer_D.step()\n\n        # Train Generator less frequently\n        if epoch % 1 == 0:  # Update Generator every step\n            optimizer_G.zero_grad(set_to_none=True)\n            gen_labels = torch.ones(current_batch_size, 1, device=device)\n            g_output = discriminator(fake_data.detach(), real_data)  # Detach fake data\n            g_loss = adversarial_loss(g_output, gen_labels)\n            g_loss.backward()\n            optimizer_G.step()\n\n    # Step the learning rate schedulers\n    scheduler_G.step()\n    scheduler_D.step()\n    \n    # Logging and checkpointing\n    if epoch % 1 == 0:\n        print(f\"Epoch [{epoch}/{num_epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n    \n    with open(loss_log_file, 'a') as f:\n        f.write(f'{epoch},{d_loss.item()},{g_loss.item()}\\n')\n    \n    if epoch % checkpoint_interval == 0 and epoch > 0:\n        save_checkpoint(epoch)\n\n# Final save\nsave_checkpoint(num_epochs - 1)\n","metadata":{"trusted":true,"tags":[]},"execution_count":null,"outputs":[],"id":"d164caa7-bfdf-4caa-baa1-a824243cbc16"},{"cell_type":"code","source":"above is the best","metadata":{},"execution_count":null,"outputs":[],"id":"3140f829-e712-4e7d-85b7-aa6f0752a954"},{"cell_type":"code","source":"tried to improve but no major","metadata":{},"execution_count":null,"outputs":[],"id":"4fe6e68f-724e-486d-a62f-65b82b573d34"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\nimport csv\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import StepLR\n\n# Configuration\ncheckpoint_dir = '/home/jovyan/FluxGAN/plots/checkpoint'\nloss_log_file = '/home/jovyan/FluxGAN/plots/loss_log.csv'\ncheckpoint_interval = 10\nnum_epochs = 1001\nbatch_size = 1024\nnoise_dim = 100\n\n# Setup CUDA device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.backends.cudnn.benchmark = True  # Enable cudnn auto-tuner\n\n# Setup directories\nos.makedirs(checkpoint_dir, exist_ok=True)\nif not os.path.exists(loss_log_file):\n    with open(loss_log_file, 'w') as f:\n        f.write('Epoch,D Loss,G Loss\\n')\n\n# Load and preprocess data\ndata = pd.read_csv('./flux_burnup_dataset.csv')\nX = data[['Enrichment (%)', 'Flux', 'Burnup']].values\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Save dataset stats for inference\ndata_min = scaler.data_min_\ndata_max = scaler.data_max_\n\n# Models for CGAN\nclass Generator(nn.Module):\n    def __init__(self, noise_dim=100):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(noise_dim + 3, 256),\n            nn.LeakyReLU(0.2),\n            nn.LayerNorm(256),\n            nn.Linear(256, 128),\n            nn.LeakyReLU(0.2),\n            nn.LayerNorm(128),\n            nn.Linear(128, 3),\n            nn.Tanh()  # Output activation function for continuous data\n        )\n        self.apply(self.init_weights)\n\n    def forward(self, z, conditions):\n        x = torch.cat([z, conditions], dim=1)\n        return self.net(x)\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(6, 256),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.LayerNorm(256),\n            nn.Linear(256, 128),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.LayerNorm(128),\n            nn.Linear(128, 1)\n        )\n        self.apply(self.init_weights)\n\n    def forward(self, x, conditions):\n        x = torch.cat([x, conditions], dim=1)\n        return self.net(x)\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n\n# Initialize models and move to GPU\ngenerator = Generator(noise_dim).to(device)\ndiscriminator = Discriminator().to(device)\n\n# Optimizers with adjusted learning rates\noptimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))  # Increase learning rate for Generator\noptimizer_D = optim.Adam(discriminator.parameters(), lr=0.00005, betas=(0.5, 0.999))  # Decrease learning rate for Discriminator\n\n# Learning Rate Scheduler\nscheduler_G = StepLR(optimizer_G, step_size=1000, gamma=0.5)\nscheduler_D = StepLR(optimizer_D, step_size=1000, gamma=0.5)\n\n# Loss function\nadversarial_loss = nn.BCEWithLogitsLoss()\n\n# Mixed Precision Scaler\nscaler = GradScaler()\n\n# Checkpoint functions\ndef save_checkpoint(epoch):\n    path = os.path.join(checkpoint_dir, f'checkpoint_{epoch}.tar')\n    torch.save({\n        'epoch': epoch,\n        'generator_state_dict': generator.state_dict(),\n        'discriminator_state_dict': discriminator.state_dict(),\n        'optimizer_G_state_dict': optimizer_G.state_dict(),\n        'optimizer_D_state_dict': optimizer_D.state_dict(),\n        'data_min': data_min,\n        'data_max': data_max\n    }, path)\n    print(f\"[Checkpoint] Saved at epoch {epoch}\")\n\ndef load_checkpoint():\n    files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.tar')]\n    if not files:\n        print(\"[Checkpoint] No checkpoint found. Starting fresh.\")\n        return 0\n    \n    latest = max(files, key=lambda f: int(f.split('_')[1].split('.')[0]))\n    path = os.path.join(checkpoint_dir, latest)\n    \n    try:\n        checkpoint = torch.load(path, map_location=device)\n        generator.load_state_dict(checkpoint['generator_state_dict'])\n        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n        optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n        optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n        print(f\"[Checkpoint] Loaded from epoch {checkpoint['epoch']}\")\n        return checkpoint['epoch'] + 1\n    except Exception as e:\n        print(f\"[Checkpoint] Error loading: {str(e)}. Starting fresh.\")\n        return 0\n\n# Load checkpoint if exists\nstart_epoch = load_checkpoint()\n\n# Convert data to CUDA tensors\ndataset = TensorDataset(torch.tensor(X_scaled, dtype=torch.float32))\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n\n# Training loop\nfor epoch in range(start_epoch, num_epochs):\n    for real_data in dataloader:\n        real_data = real_data[0].to(device, non_blocking=True)\n        current_batch_size = real_data.size(0)\n        \n        # Train Discriminator\n        optimizer_D.zero_grad(set_to_none=True)\n        \n        # Real data with label smoothing\n        real_labels = torch.full((current_batch_size, 1), 0.9, device=device)  # Smoothing real labels\n        real_output = discriminator(real_data, real_data)\n        d_loss_real = adversarial_loss(real_output, real_labels)\n        \n        # Fake data with label smoothing\n        z = torch.randn(current_batch_size, noise_dim, device=device)\n        fake_data = generator(z, real_data)\n        fake_labels = torch.full((current_batch_size, 1), 0.1, device=device)  # Smoothing fake labels\n        fake_output = discriminator(fake_data.detach(), real_data)\n        d_loss_fake = adversarial_loss(fake_output, fake_labels)\n        \n        # Total discriminator loss\n        d_loss = (d_loss_real + d_loss_fake) / 2\n        d_loss.backward()\n        optimizer_D.step()\n\n        # Train Generator less frequently (after every 2 discriminator updates)\n        if epoch % 2 == 0:  # Update Generator every 2 discriminator steps\n            optimizer_G.zero_grad(set_to_none=True)\n            gen_labels = torch.ones(current_batch_size, 1, device=device)\n            g_output = discriminator(fake_data.detach(), real_data)  # Detach fake data\n            g_loss = adversarial_loss(g_output, gen_labels)\n            g_loss.backward()\n            optimizer_G.step()\n\n    # Step the learning rate schedulers\n    scheduler_G.step()\n    scheduler_D.step()\n    \n    # Logging and checkpointing\n    if epoch % 1 == 0:\n        print(f\"Epoch [{epoch}/{num_epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n    \n    with open(loss_log_file, 'a') as f:\n        f.write(f'{epoch},{d_loss.item()},{g_loss.item()}\\n')\n    \n    if epoch % checkpoint_interval == 0 and epoch > 0:\n        save_checkpoint(epoch)\n\n# Final save\nsave_checkpoint(num_epochs - 1)\n","metadata":{},"execution_count":null,"outputs":[],"id":"4e377879-76a2-4612-a52b-4e13aa5fc025"},{"cell_type":"code","source":"ON WORK","metadata":{},"execution_count":null,"outputs":[],"id":"960efb48-4149-4346-bf23-a38c89da860e"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\nimport csv\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.utils.spectral_norm as spectral_norm\n\n# Configuration\ncheckpoint_dir = '/home/jovyan/FluxGAN/plots/checkpoint'\nloss_log_file = '/home/jovyan/FluxGAN/plots/loss_log.csv'\ncheckpoint_interval = 1000\nnum_epochs = 30001\nbatch_size = 512\nnoise_dim = 100\n\n# Setup CUDA device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.backends.cudnn.benchmark = True  # Enable cudnn auto-tuner\n\n# Setup directories\nos.makedirs(checkpoint_dir, exist_ok=True)\nif not os.path.exists(loss_log_file):\n    with open(loss_log_file, 'w') as f:\n        f.write('Epoch,D Loss,G Loss\\n')\n\n# Load and preprocess data\ndata = pd.read_csv('./flux_burnup_dataset.csv')\nX = data[['Enrichment (%)', 'Flux', 'Burnup']].values\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Save dataset stats for inference\ndata_min = scaler.data_min_\ndata_max = scaler.data_max_\n\n# Generator Model\nclass Generator(nn.Module):\n    def __init__(self, noise_dim=100):\n        super().__init__()\n        self.fc1 = nn.Linear(noise_dim + 3, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 256)\n        self.fc4 = nn.Linear(256, 3)\n        \n        # Initialize weights\n        self.apply(self.init_weights)\n\n    def forward(self, z, conditions):\n        x = torch.cat([z, conditions], dim=1)\n        x = nn.LeakyReLU(0.2)(self.fc1(x))\n        residual = x\n        x = nn.LeakyReLU(0.2)(self.fc2(x))\n        x = nn.LeakyReLU(0.2)(self.fc3(x))\n        x += residual\n        x = self.fc4(x)\n        return torch.tanh(x)\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n\n# Discriminator Model\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            spectral_norm(nn.Linear(6, 256)),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.LayerNorm(256),\n            spectral_norm(nn.Linear(256, 128)),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n            nn.LayerNorm(128),\n            spectral_norm(nn.Linear(128, 1))\n        )\n        self.apply(self.init_weights)\n\n    def forward(self, x, conditions):\n        x = torch.cat([x, conditions], dim=1)\n        return self.net(x)\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n\n# Initialize models\ngenerator = Generator(noise_dim).to(device)\ndiscriminator = Discriminator().to(device)\n\n# Optimizers\noptimizer_G = optim.RMSprop(generator.parameters(), lr=0.0002)\noptimizer_D = optim.RMSprop(discriminator.parameters(), lr=0.00005)\n\n# Learning Rate Schedulers\nscheduler_G = StepLR(optimizer_G, step_size=1000, gamma=0.5)\nscheduler_D = StepLR(optimizer_D, step_size=1000, gamma=0.5)\n\n# Loss functions for WGAN\ndef wgan_discriminator_loss(real_output, fake_output):\n    return torch.mean(fake_output) - torch.mean(real_output)\n\ndef wgan_generator_loss(fake_output):\n    return -torch.mean(fake_output)\n\n# Gradient Penalty for WGAN-GP\ndef calc_gradient_penalty(discriminator, real_data, fake_data, conditions, batch_size, device):\n    epsilon = torch.rand(batch_size, 1, device=device)\n    interpolated_data = epsilon * real_data + (1 - epsilon) * fake_data\n    interpolated_data.requires_grad_(True)\n\n    d_interpolated = discriminator(interpolated_data, conditions)\n    \n    gradients = torch.autograd.grad(\n        outputs=d_interpolated,\n        inputs=interpolated_data,\n        grad_outputs=torch.ones_like(d_interpolated),\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True\n    )[0]\n    \n    gradients = gradients.view(batch_size, -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty\n\n# Checkpoint functions\ndef save_checkpoint(epoch):\n    path = os.path.join(checkpoint_dir, f'checkpoint_{epoch}.tar')\n    torch.save({\n        'epoch': epoch,\n        'generator_state_dict': generator.state_dict(),\n        'discriminator_state_dict': discriminator.state_dict(),\n        'optimizer_G_state_dict': optimizer_G.state_dict(),\n        'optimizer_D_state_dict': optimizer_D.state_dict(),\n        'data_min': data_min,\n        'data_max': data_max\n    }, path)\n    print(f\"[Checkpoint] Saved at epoch {epoch}\")\n\ndef load_checkpoint():\n    files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.tar')]\n    if not files:\n        print(\"[Checkpoint] No checkpoint found. Starting fresh.\")\n        return 0\n    \n    latest = max(files, key=lambda f: int(f.split('_')[1].split('.')[0]))\n    path = os.path.join(checkpoint_dir, latest)\n    \n    try:\n        checkpoint = torch.load(path, map_location=device)\n        generator.load_state_dict(checkpoint['generator_state_dict'])\n        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n        optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n        optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n        print(f\"[Checkpoint] Loaded from epoch {checkpoint['epoch']}\")\n        return checkpoint['epoch'] + 1\n    except Exception as e:\n        print(f\"[Checkpoint] Error loading: {str(e)}. Starting fresh.\")\n        return 0\n\n# Load checkpoint if exists\nstart_epoch = load_checkpoint()\n\n# Convert data to CUDA tensors\ndataset = TensorDataset(torch.tensor(X_scaled, dtype=torch.float32))\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n\n# Training loop\nfor epoch in range(start_epoch, num_epochs):\n    for real_data in dataloader:\n        real_data = real_data[0].to(device, non_blocking=True)\n        current_batch_size = real_data.size(0)\n        \n        # Generate noise and fake data\n        noise = torch.randn(current_batch_size, noise_dim, device=device)\n        fake_data = generator(noise, real_data)\n        \n        # Train Discriminator\n        optimizer_D.zero_grad()\n        \n        # Real data\n        real_output = discriminator(real_data, real_data)\n        # Fake data (detached)\n        fake_output = discriminator(fake_data.detach(), real_data)\n        \n        # WGAN loss\n        d_loss = wgan_discriminator_loss(real_output, fake_output)\n        \n        # Gradient penalty\n        grad_penalty = calc_gradient_penalty(\n            discriminator, \n            real_data, \n            fake_data.detach(), \n            real_data, \n            current_batch_size, \n            device\n        )\n        d_loss += 10 * grad_penalty\n        \n        d_loss.backward()\n        optimizer_D.step()\n\n        # Train Generator\n        optimizer_G.zero_grad()\n        \n        # Generate new fake data for generator training\n        fake_data = generator(noise, real_data)\n        g_output = discriminator(fake_data, real_data)\n        g_loss = wgan_generator_loss(g_output)\n        \n        g_loss.backward()\n        optimizer_G.step()\n\n    # Update learning rates\n    scheduler_G.step()\n    scheduler_D.step()\n    \n    # Logging\n    if epoch % 1 == 0:\n        print(f\"Epoch [{epoch}/{num_epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n    \n    with open(loss_log_file, 'a') as f:\n        f.write(f'{epoch},{d_loss.item()},{g_loss.item()}\\n')\n    \n    # Checkpointing\n    if epoch % checkpoint_interval == 0 and epoch > 0:\n        save_checkpoint(epoch)\n\n# Final save\nsave_checkpoint(num_epochs - 1)","metadata":{"trusted":true,"tags":[]},"execution_count":null,"outputs":[],"id":"26e88b65-8796-4535-90b1-8e9dce3966a8"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"0ca418f8-c44a-4b1e-a735-c932a8d4851c"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import StepLR\n\n# Configuration\ncheckpoint_dir = '/home/jovyan/FluxGAN/plots/checkpoint'\nloss_log_file = '/home/jovyan/FluxGAN/plots/loss_log.csv'\ncheckpoint_interval = 1000\nnum_epochs = 30001\nbatch_size = 512\nnoise_dim = 100\nlabel_flip_rate = 0.05   # 5% label flipping\n\n# Setup CUDA device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.backends.cudnn.benchmark = True\n\n# Setup directories\nos.makedirs(checkpoint_dir, exist_ok=True)\nif not os.path.exists(loss_log_file):\n    with open(loss_log_file, 'w') as f:\n        f.write('Epoch,D Loss,G Loss,GenMean,GenStd\\n')\n\n# Load and preprocess data\ndata = pd.read_csv('./flux_burnup_dataset.csv')\nX = data[['Enrichment (%)', 'Flux', 'Burnup']].values\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Save dataset stats for inference\ndata_min = scaler.data_min_\ndata_max = scaler.data_max_\n\n# Models for Vanilla GAN\nclass Generator(nn.Module):\n    def __init__(self, noise_dim=100):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.utils.spectral_norm(nn.Linear(noise_dim, 256)),\n            nn.LeakyReLU(0.2),\n            nn.LayerNorm(256),\n            nn.utils.spectral_norm(nn.Linear(256, 128)),\n            nn.LeakyReLU(0.2),\n            nn.LayerNorm(128),\n            nn.utils.spectral_norm(nn.Linear(128, 3)),\n            nn.Tanh()\n        )\n        self.apply(self.init_weights)\n\n    def forward(self, z):\n        return self.net(z)\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.utils.spectral_norm(nn.Linear(3, 256)),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.1),\n            nn.LayerNorm(256),\n            nn.utils.spectral_norm(nn.Linear(256, 128)),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.1),\n            nn.LayerNorm(128),\n            nn.utils.spectral_norm(nn.Linear(128, 1))\n        )\n        self.apply(self.init_weights)\n\n    def forward(self, x):\n        return self.net(x)\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_normal_(m.weight)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n\n# Initialize models and move to GPU\ngenerator = Generator(noise_dim).to(device)\ndiscriminator = Discriminator().to(device)\n\n# Optimizers with weight decay and adjusted learning rates\noptimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999), weight_decay=1e-5)\noptimizer_D = optim.Adam(discriminator.parameters(), lr=0.00005, betas=(0.5, 0.999), weight_decay=1e-5)\n\n# Learning Rate Scheduler (gentler decay)\nscheduler_G = StepLR(optimizer_G, step_size=1000, gamma=0.8)\nscheduler_D = StepLR(optimizer_D, step_size=1000, gamma=0.8)\n\n# Loss function\nadversarial_loss = nn.BCEWithLogitsLoss()\n\n# Mixed Precision Scaler\nscaler = GradScaler()\n\n# Checkpoint functions\ndef save_checkpoint(epoch):\n    path = os.path.join(checkpoint_dir, f'checkpoint_{epoch}.tar')\n    torch.save({\n        'epoch': epoch,\n        'generator_state_dict': generator.state_dict(),\n        'discriminator_state_dict': discriminator.state_dict(),\n        'optimizer_G_state_dict': optimizer_G.state_dict(),\n        'optimizer_D_state_dict': optimizer_D.state_dict(),\n        'data_min': data_min,\n        'data_max': data_max\n    }, path)\n    print(f\"[Checkpoint] Saved at epoch {epoch}\")\n\ndef load_checkpoint():\n    files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.tar')]\n    if not files:\n        print(\"[Checkpoint] No checkpoint found. Starting fresh.\")\n        return 0\n\n    latest = max(files, key=lambda f: int(f.split('_')[1].split('.')[0]))\n    path = os.path.join(checkpoint_dir, latest)\n\n    try:\n        checkpoint = torch.load(path, map_location=device)\n        generator.load_state_dict(checkpoint['generator_state_dict'])\n        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n        optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n        optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n        print(f\"[Checkpoint] Loaded from epoch {checkpoint['epoch']}\")\n        return checkpoint['epoch'] + 1\n    except Exception as e:\n        print(f\"[Checkpoint] Error loading: {str(e)}. Starting fresh.\")\n        return 0\n\n# Load checkpoint if exists\nstart_epoch = load_checkpoint()\n\n# Convert data to CUDA tensors\ndataset = TensorDataset(torch.tensor(X_scaled, dtype=torch.float32))\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n\n# Training loop\nfor epoch in range(start_epoch, num_epochs):\n    for real_data in dataloader:\n        real_data = real_data[0].to(device, non_blocking=True)\n        current_batch_size = real_data.size(0)\n\n        # ====== Train Discriminator ======\n        optimizer_D.zero_grad(set_to_none=True)\n        with autocast():\n            real_labels = torch.full((current_batch_size, 1), 0.9, device=device)\n            fake_labels = torch.zeros((current_batch_size, 1), device=device)\n            n_flip = int(label_flip_rate * current_batch_size)\n            if n_flip > 0:\n                idx_flip = torch.randperm(current_batch_size)[:n_flip]\n                real_labels[idx_flip] = 0\n                fake_labels[idx_flip] = 1\n\n            real_output = discriminator(real_data)\n            d_loss_real = adversarial_loss(real_output, real_labels)\n\n            z = torch.randn(current_batch_size, noise_dim, device=device)\n            fake_data = generator(z)\n            fake_output = discriminator(fake_data.detach())\n            d_loss_fake = adversarial_loss(fake_output, fake_labels)\n            d_loss = (d_loss_real + d_loss_fake) / 2\n\n        scaler.scale(d_loss).backward()\n        scaler.step(optimizer_D)\n        scaler.update()\n\n        # ====== Train Generator (1:1 ratio) ======\n        optimizer_G.zero_grad(set_to_none=True)\n        with autocast():\n            gen_labels = torch.ones(current_batch_size, 1, device=device)\n            g_output = discriminator(fake_data)\n            g_loss = adversarial_loss(g_output, gen_labels)\n        scaler.scale(g_loss).backward()\n        scaler.step(optimizer_G)\n        scaler.update()\n\n    # Step the learning rate schedulers\n    scheduler_G.step()\n    scheduler_D.step()\n\n    # Logging and checkpointing\n    if epoch % 1 == 0:\n        with torch.no_grad():\n            z_log = torch.randn(batch_size, noise_dim, device=device)\n            gen_samples = generator(z_log).cpu().numpy()\n            gen_mean, gen_std = gen_samples.mean(), gen_samples.std()\n        print(f\"Epoch [{epoch}/{num_epochs}] | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f} | GenMean: {gen_mean:.3f} | GenStd: {gen_std:.3f}\")\n\n        with open(loss_log_file, 'a') as f:\n            f.write(f'{epoch},{d_loss.item()},{g_loss.item()},{gen_mean},{gen_std}\\n')\n\n    if epoch % checkpoint_interval == 0 and epoch > 0:\n        save_checkpoint(epoch)\n\n# Final save\nsave_checkpoint(num_epochs - 1)\n","metadata":{"trusted":true,"tags":[]},"execution_count":7,"outputs":[{"name":"stdout","text":"[Checkpoint] Error loading: Error(s) in loading state_dict for Generator:\n\tsize mismatch for net.0.weight_orig: copying a param with shape torch.Size([256, 103]) from checkpoint, the shape in current model is torch.Size([256, 100]).\n\tsize mismatch for net.0.weight_v: copying a param with shape torch.Size([103]) from checkpoint, the shape in current model is torch.Size([100]).. Starting fresh.\nEpoch [0/30001] | D Loss: 0.8873 | G Loss: 0.6894 | GenMean: 0.124 | GenStd: 0.498\nEpoch [1/30001] | D Loss: 0.6413 | G Loss: 0.9067 | GenMean: 0.154 | GenStd: 0.506\nEpoch [2/30001] | D Loss: 0.6079 | G Loss: 1.0397 | GenMean: 0.258 | GenStd: 0.507\nEpoch [3/30001] | D Loss: 0.6692 | G Loss: 0.9263 | GenMean: 0.405 | GenStd: 0.451\nEpoch [4/30001] | D Loss: 0.7189 | G Loss: 0.7875 | GenMean: 0.509 | GenStd: 0.387\nEpoch [5/30001] | D Loss: 0.7095 | G Loss: 0.8129 | GenMean: 0.563 | GenStd: 0.341\nEpoch [6/30001] | D Loss: 0.7105 | G Loss: 0.8179 | GenMean: 0.599 | GenStd: 0.322\nEpoch [7/30001] | D Loss: 0.6867 | G Loss: 0.8215 | GenMean: 0.577 | GenStd: 0.345\nEpoch [8/30001] | D Loss: 0.7129 | G Loss: 0.8325 | GenMean: 0.560 | GenStd: 0.344\nEpoch [9/30001] | D Loss: 0.7085 | G Loss: 0.8286 | GenMean: 0.535 | GenStd: 0.353\nEpoch [10/30001] | D Loss: 0.6975 | G Loss: 0.8338 | GenMean: 0.522 | GenStd: 0.348\nEpoch [11/30001] | D Loss: 0.7040 | G Loss: 0.8499 | GenMean: 0.538 | GenStd: 0.344\nEpoch [12/30001] | D Loss: 0.6814 | G Loss: 0.8148 | GenMean: 0.546 | GenStd: 0.334\nEpoch [13/30001] | D Loss: 0.7024 | G Loss: 0.8024 | GenMean: 0.533 | GenStd: 0.319\nEpoch [14/30001] | D Loss: 0.6994 | G Loss: 0.8086 | GenMean: 0.492 | GenStd: 0.318\nEpoch [15/30001] | D Loss: 0.7042 | G Loss: 0.8065 | GenMean: 0.449 | GenStd: 0.332\nEpoch [16/30001] | D Loss: 0.7075 | G Loss: 0.8320 | GenMean: 0.421 | GenStd: 0.316\nEpoch [17/30001] | D Loss: 0.6955 | G Loss: 0.8128 | GenMean: 0.421 | GenStd: 0.311\nEpoch [18/30001] | D Loss: 0.6973 | G Loss: 0.8109 | GenMean: 0.453 | GenStd: 0.306\nEpoch [19/30001] | D Loss: 0.6998 | G Loss: 0.8145 | GenMean: 0.469 | GenStd: 0.306\nEpoch [20/30001] | D Loss: 0.6869 | G Loss: 0.8157 | GenMean: 0.476 | GenStd: 0.304\nEpoch [21/30001] | D Loss: 0.6988 | G Loss: 0.8085 | GenMean: 0.480 | GenStd: 0.318\nEpoch [22/30001] | D Loss: 0.6977 | G Loss: 0.8004 | GenMean: 0.489 | GenStd: 0.316\nEpoch [23/30001] | D Loss: 0.6942 | G Loss: 0.8245 | GenMean: 0.494 | GenStd: 0.328\nEpoch [24/30001] | D Loss: 0.6974 | G Loss: 0.8113 | GenMean: 0.496 | GenStd: 0.330\nEpoch [25/30001] | D Loss: 0.6954 | G Loss: 0.8077 | GenMean: 0.506 | GenStd: 0.337\nEpoch [26/30001] | D Loss: 0.6910 | G Loss: 0.8166 | GenMean: 0.519 | GenStd: 0.348\nEpoch [27/30001] | D Loss: 0.6911 | G Loss: 0.8072 | GenMean: 0.523 | GenStd: 0.341\nEpoch [28/30001] | D Loss: 0.6925 | G Loss: 0.7941 | GenMean: 0.515 | GenStd: 0.338\nEpoch [29/30001] | D Loss: 0.6982 | G Loss: 0.8061 | GenMean: 0.522 | GenStd: 0.339\nEpoch [30/30001] | D Loss: 0.6899 | G Loss: 0.8177 | GenMean: 0.507 | GenStd: 0.333\nEpoch [31/30001] | D Loss: 0.6899 | G Loss: 0.8044 | GenMean: 0.504 | GenStd: 0.341\nEpoch [32/30001] | D Loss: 0.6922 | G Loss: 0.8020 | GenMean: 0.493 | GenStd: 0.335\nEpoch [33/30001] | D Loss: 0.6970 | G Loss: 0.7928 | GenMean: 0.495 | GenStd: 0.340\nEpoch [34/30001] | D Loss: 0.6904 | G Loss: 0.7993 | GenMean: 0.502 | GenStd: 0.339\nEpoch [35/30001] | D Loss: 0.6966 | G Loss: 0.8038 | GenMean: 0.511 | GenStd: 0.338\nEpoch [36/30001] | D Loss: 0.6920 | G Loss: 0.7958 | GenMean: 0.510 | GenStd: 0.335\nEpoch [37/30001] | D Loss: 0.6823 | G Loss: 0.7929 | GenMean: 0.511 | GenStd: 0.336\nEpoch [38/30001] | D Loss: 0.6941 | G Loss: 0.8013 | GenMean: 0.498 | GenStd: 0.332\nEpoch [39/30001] | D Loss: 0.6882 | G Loss: 0.7956 | GenMean: 0.504 | GenStd: 0.340\nEpoch [40/30001] | D Loss: 0.6914 | G Loss: 0.7863 | GenMean: 0.487 | GenStd: 0.342\nEpoch [41/30001] | D Loss: 0.6909 | G Loss: 0.8103 | GenMean: 0.503 | GenStd: 0.347\nEpoch [42/30001] | D Loss: 0.6827 | G Loss: 0.8005 | GenMean: 0.525 | GenStd: 0.347\nEpoch [43/30001] | D Loss: 0.6919 | G Loss: 0.7996 | GenMean: 0.505 | GenStd: 0.338\nEpoch [44/30001] | D Loss: 0.6880 | G Loss: 0.7968 | GenMean: 0.506 | GenStd: 0.341\nEpoch [45/30001] | D Loss: 0.6880 | G Loss: 0.7965 | GenMean: 0.505 | GenStd: 0.341\nEpoch [46/30001] | D Loss: 0.6958 | G Loss: 0.8031 | GenMean: 0.505 | GenStd: 0.343\nEpoch [47/30001] | D Loss: 0.6939 | G Loss: 0.8054 | GenMean: 0.499 | GenStd: 0.351\nEpoch [48/30001] | D Loss: 0.6889 | G Loss: 0.8020 | GenMean: 0.516 | GenStd: 0.352\nEpoch [49/30001] | D Loss: 0.6940 | G Loss: 0.7953 | GenMean: 0.507 | GenStd: 0.346\nEpoch [50/30001] | D Loss: 0.6885 | G Loss: 0.7933 | GenMean: 0.510 | GenStd: 0.352\nEpoch [51/30001] | D Loss: 0.6873 | G Loss: 0.7974 | GenMean: 0.516 | GenStd: 0.355\nEpoch [52/30001] | D Loss: 0.6897 | G Loss: 0.7942 | GenMean: 0.514 | GenStd: 0.347\nEpoch [53/30001] | D Loss: 0.6904 | G Loss: 0.8012 | GenMean: 0.519 | GenStd: 0.352\nEpoch [54/30001] | D Loss: 0.6887 | G Loss: 0.7974 | GenMean: 0.511 | GenStd: 0.353\nEpoch [55/30001] | D Loss: 0.6911 | G Loss: 0.7926 | GenMean: 0.513 | GenStd: 0.351\nEpoch [56/30001] | D Loss: 0.6905 | G Loss: 0.8021 | GenMean: 0.505 | GenStd: 0.350\nEpoch [57/30001] | D Loss: 0.6897 | G Loss: 0.8068 | GenMean: 0.519 | GenStd: 0.355\nEpoch [58/30001] | D Loss: 0.6942 | G Loss: 0.7944 | GenMean: 0.516 | GenStd: 0.353\nEpoch [59/30001] | D Loss: 0.6891 | G Loss: 0.8010 | GenMean: 0.518 | GenStd: 0.352\nEpoch [60/30001] | D Loss: 0.6919 | G Loss: 0.8007 | GenMean: 0.511 | GenStd: 0.347\nEpoch [61/30001] | D Loss: 0.6900 | G Loss: 0.8036 | GenMean: 0.525 | GenStd: 0.349\nEpoch [62/30001] | D Loss: 0.6892 | G Loss: 0.8023 | GenMean: 0.508 | GenStd: 0.349\nEpoch [63/30001] | D Loss: 0.6912 | G Loss: 0.7980 | GenMean: 0.517 | GenStd: 0.351\nEpoch [64/30001] | D Loss: 0.6882 | G Loss: 0.7961 | GenMean: 0.510 | GenStd: 0.347\nEpoch [65/30001] | D Loss: 0.6871 | G Loss: 0.8002 | GenMean: 0.511 | GenStd: 0.346\nEpoch [66/30001] | D Loss: 0.6916 | G Loss: 0.7927 | GenMean: 0.505 | GenStd: 0.347\nEpoch [67/30001] | D Loss: 0.6865 | G Loss: 0.8004 | GenMean: 0.502 | GenStd: 0.349\nEpoch [68/30001] | D Loss: 0.6898 | G Loss: 0.7945 | GenMean: 0.513 | GenStd: 0.342\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 185\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m    184\u001b[0m     gen_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(current_batch_size, \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 185\u001b[0m     g_output \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     g_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(g_output, gen_labels)\n\u001b[1;32m    187\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(g_loss)\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[7], line 83\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/torch/nn/modules/module.py:1571\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1567\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward pre-hook must return None or a tuple \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1568\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_kwargs_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1569\u001b[0m             )\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1571\u001b[0m     args_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1572\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1573\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_result, \u001b[38;5;28mtuple\u001b[39m):\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/torch/nn/utils/spectral_norm.py:105\u001b[0m, in \u001b[0;36mSpectralNorm.__call__\u001b[0;34m(self, module, inputs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, module: Module, inputs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_power_iteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/torch/nn/utils/spectral_norm.py:84\u001b[0m, in \u001b[0;36mSpectralNorm.compute_weight\u001b[0;34m(self, module, do_power_iteration)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_power_iterations):\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;66;03m# Spectral norm of weight equals to `u^T W v`, where `u` and `v`\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;66;03m# are the first left and right singular vectors.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;66;03m# This power iteration produces approximations of `u` and `v`.\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m         v \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_mat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m         u \u001b[38;5;241m=\u001b[39m normalize(torch\u001b[38;5;241m.\u001b[39mmv(weight_mat, v), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps, out\u001b[38;5;241m=\u001b[39mu)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_power_iterations \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;66;03m# See above on why we need to clone\u001b[39;00m\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/torch/nn/functional.py:4783\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(input, p, dim, eps, out)\u001b[0m\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m/\u001b[39m denom\n\u001b[1;32m   4782\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4783\u001b[0m     denom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp_min_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;28minput\u001b[39m, denom, out\u001b[38;5;241m=\u001b[39mout)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"id":"b113a764-885a-4411-b799-db41897162bc"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"4abea393-9b17-4c2a-8fcb-388669c19d64"}]}
