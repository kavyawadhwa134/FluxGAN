{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the generator network\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Linear(2, 16),  # Input size is 2 (latent space)\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 16),\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 2)  # Output size should be 2 for 2D data\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n# Define the discriminator network\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Linear(2, 16),  # Input size should match the generated data size\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 16),\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 1)  # Output size is 1 for binary classification\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n# Create instances of the generator and discriminator and move to device\ngenerator = Generator().to(device)\ndiscriminator = Discriminator().to(device)\n\n# Loss function\ncriterion = nn.BCEWithLogitsLoss()\n\n# Optimizers\ngen_optimizer = optim.RMSprop(generator.parameters(), lr=0.001)\ndisc_optimizer = optim.RMSprop(discriminator.parameters(), lr=0.001)\n\n# Function to generate noise\ndef sample_Z(m, n):\n    return torch.Tensor(np.random.uniform(-1., 1., size=[m, n])).to(device)\n\n# Load training data from m_hist.txt\ndef load_data(file_path):\n    # Load only the first two columns to avoid dimension mismatch\n    data = np.loadtxt(file_path, usecols=[0, 1])  \n    return torch.Tensor(data).to(device)\n\n# Load your real training data\nreal_data_file = '/home/jovyan/plots-gan/basic-gans/code/m_hist.txt'  # Full path to your data file\nx_plot = load_data(real_data_file)\n\n# Training parameters\nbatch_size = min(256, len(x_plot))  # Adjust batch size to be smaller than or equal to the number of data points\nnd_steps = 10\nng_steps = 10\nnum_iterations = 12001  # Set to 12000 iterations\n\n# Path to the loss log file\nloss_log_file = '../plots/loss_log.csv'\n\n# Write the header of the CSV file\nwith open(loss_log_file, 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow(['Iteration', 'Discriminator Loss', 'Generator Loss'])\n\n# Training loop\nfor i in range(num_iterations):\n    for _ in range(nd_steps):\n        # Sample a batch of real data\n        indices = np.random.choice(len(x_plot), batch_size, replace=False)\n        X_batch = x_plot[indices]\n        Z_batch = sample_Z(batch_size, 2)\n\n        # Real data labels are 1, generated data labels are 0\n        real_labels = torch.ones(batch_size, 1).to(device)\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n\n        # Compute discriminator loss on real data\n        real_outputs = discriminator(X_batch)\n        real_loss = criterion(real_outputs, real_labels)\n\n        # Compute discriminator loss on fake data\n        fake_data = generator(Z_batch)\n        fake_outputs = discriminator(fake_data.detach())\n        fake_loss = criterion(fake_outputs, fake_labels)\n\n        # Total discriminator loss\n        disc_loss = real_loss + fake_loss\n\n        # Backpropagation and optimization for discriminator\n        disc_optimizer.zero_grad()\n        disc_loss.backward()\n        disc_optimizer.step()\n\n    for _ in range(ng_steps):\n        Z_batch = sample_Z(batch_size, 2)\n        fake_data = generator(Z_batch)\n        fake_outputs = discriminator(fake_data)\n\n        # Generator loss\n        gen_loss = criterion(fake_outputs, real_labels)\n\n        # Backpropagation and optimization for generator\n        gen_optimizer.zero_grad()\n        gen_loss.backward()\n        gen_optimizer.step()\n\n    # Save the losses to the CSV file\n    with open(loss_log_file, 'a', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([i, disc_loss.item(), gen_loss.item()])\n\n    print(f\"Iterations: {i}\\t Discriminator loss: {disc_loss.item():.4f}\\t Generator loss: {gen_loss.item():.4f}\")\n\n    # Plotting every 1000 iterations\n    if i % 1000 == 0:\n        plt.figure()\n        with torch.no_grad():\n            g_plot = generator(sample_Z(batch_size, 2)).cpu().numpy()  # Generate new data for plotting\n        plt.scatter(x_plot.cpu()[:, 0], x_plot.cpu()[:, 1], color='blue', label='Real Data', alpha=0.6)\n        plt.scatter(g_plot[:, 0], g_plot[:, 1], color='orange', label='Generated Data', alpha=0.6)\n\n        plt.legend()\n        plt.title(f'Samples at Iteration {i}', fontsize=14)\n        plt.xlabel('X-axis Label', fontsize=12)\n        plt.ylabel('Y-axis Label', fontsize=12)\n        plt.grid(True)\n        plt.tight_layout()\n        plt.savefig(f'../plots/iterations/iteration_{i}.png', dpi=300)\n        plt.close()\n","metadata":{"tags":[],"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Load your real training data\u001b[39;00m\n\u001b[1;32m     63\u001b[0m real_data_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jovyan/plots-gan/basic-gans/code/m_hist.txt\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Full path to your data file\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m x_plot \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_data_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Training parameters\u001b[39;00m\n\u001b[1;32m     67\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;28mlen\u001b[39m(x_plot))  \u001b[38;5;66;03m# Adjust batch size to be smaller than or equal to the number of data points\u001b[39;00m\n","Cell \u001b[0;32mIn[2], line 59\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Load only the first two columns to avoid dimension mismatch\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor(data)\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1381\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1379\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1381\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:997\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    995\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 997\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    999\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/numpy/lib/_datasource.py:192\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/numpy/lib/_datasource.py:532\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    530\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 532\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: /home/jovyan/plots-gan/basic-gans/code/m_hist.txt not found."],"ename":"FileNotFoundError","evalue":"/home/jovyan/plots-gan/basic-gans/code/m_hist.txt not found.","output_type":"error"}],"id":"a7d39ded-2836-46a4-850f-72181eae8eb0"},{"cell_type":"code","source":"","metadata":{"tags":[]},"execution_count":null,"outputs":[],"id":"173f5b7f-ac97-4e3b-a5b1-7b7ed963a301"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- Hyperparameters ---\nlatent_dim = 32  # latent space dimension\nhidden_dim = 128  # neurons per hidden layer\nbatch_size = 128\nnum_iterations = 30000\nlr = 0.0002\nbeta1, beta2 = 0.5, 0.999  # Adam betas\n\n# --- Generator ---\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, output_dim):\n        super(Generator, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.LeakyReLU(0.2),\n\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.LeakyReLU(0.2),\n\n            nn.Linear(hidden_dim, output_dim),\n            # No activation here if output can have negative or large values\n        )\n\n    def forward(self, z):\n        return self.net(z)\n\n# --- Discriminator ---\nclass Discriminator(nn.Module):\n    def __init__(self, input_dim):\n        super(Discriminator, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.LeakyReLU(0.2),\n\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.LeakyReLU(0.2),\n\n            nn.Linear(hidden_dim, 1)\n            # Output logits (no sigmoid here because we use BCEWithLogitsLoss)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n# --- Load and Prepare Data ---\ndef load_data(file_path):\n    data = np.loadtxt(file_path, delimiter=',')\n    data_tensor = torch.tensor(data, dtype=torch.float32)\n    # Normalize data between -1 and 1 for stable training\n    min_val = data_tensor.min()\n    max_val = data_tensor.max()\n    data_tensor = 2 * (data_tensor - min_val) / (max_val - min_val) - 1\n    return data_tensor.to(device), min_val, max_val\n\nreal_data_file = '/home/jovyan/Muon_decay_GAN/GAN/code/m_hist.txt'\nreal_data, data_min, data_max = load_data(real_data_file)\n\ndataset = TensorDataset(real_data)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n\ndata_dim = real_data.shape[1]  # output dimension\n\n# --- Instantiate models ---\ngenerator = Generator(latent_dim, data_dim).to(device)\ndiscriminator = Discriminator(data_dim).to(device)\n\n# --- Optimizers ---\ngen_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\ndisc_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n\n# --- Loss ---\ncriterion = nn.BCEWithLogitsLoss()\n\n# --- Training Loop ---\nfor iteration in range(num_iterations):\n    for real_batch_tuple in dataloader:\n        real_batch = real_batch_tuple[0].to(device)\n\n        batch_size_curr = real_batch.size(0)\n\n        # Labels with smoothing\n        real_labels = torch.full((batch_size_curr, 1), 0.9, device=device)\n        fake_labels = torch.zeros((batch_size_curr, 1), device=device)\n\n        # --- Train Discriminator ---\n        disc_optimizer.zero_grad()\n\n        # Real data\n        real_preds = discriminator(real_batch)\n        real_loss = criterion(real_preds, real_labels)\n\n        # Fake data\n        z = torch.randn(batch_size_curr, latent_dim, device=device)\n        fake_data = generator(z)\n        fake_preds = discriminator(fake_data.detach())\n        fake_loss = criterion(fake_preds, fake_labels)\n\n        disc_loss = real_loss + fake_loss\n        disc_loss.backward()\n        disc_optimizer.step()\n\n        # --- Train Generator ---\n        gen_optimizer.zero_grad()\n\n        z = torch.randn(batch_size_curr, latent_dim, device=device)\n        fake_data = generator(z)\n        fake_preds = discriminator(fake_data)\n        # We want generator to fool discriminator, so labels are real\n        gen_loss = criterion(fake_preds, real_labels)\n\n        gen_loss.backward()\n        gen_optimizer.step()\n\n    if iteration % 500 == 0:\n        print(f\"Iter {iteration} | Disc Loss: {disc_loss.item():.4f} | Gen Loss: {gen_loss.item():.4f}\")\n\n        # Generate some samples for visualization\n        generator.eval()\n        with torch.no_grad():\n            z = torch.randn(batch_size, latent_dim, device=device)\n            generated_samples = generator(z).cpu()\n\n            # Rescale to original range for plotting\n            generated_samples = (generated_samples + 1) / 2  # scale back to [0,1]\n            generated_samples = generated_samples * (data_max - data_min) + data_min\n\n        plt.figure(figsize=(6, 6))\n        plt.scatter(real_data.cpu()[:, 0], real_data.cpu()[:, 1], alpha=0.3, label=\"Real Data\")\n        plt.scatter(generated_samples[:, 0], generated_samples[:, 1], alpha=0.3, label=\"Generated Data\")\n        plt.legend()\n        plt.title(f\"Flux GAN Samples at Iter {iteration}\")\n        plt.show()\n        generator.train()\n","metadata":{"tags":[],"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_tensor\u001b[38;5;241m.\u001b[39mto(device), min_val, max_val\n\u001b[1;32m     67\u001b[0m real_data_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jovyan/Muon_decay_GAN/GAN/code/m_hist.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 68\u001b[0m real_data, data_min, data_max \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_data_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(real_data)\n\u001b[1;32m     71\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","Cell \u001b[0;32mIn[3], line 59\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[0;32m---> 59\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     data_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(data, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Normalize data between -1 and 1 for stable training\u001b[39;00m\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1381\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1379\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1381\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n","File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:1021\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     data \u001b[38;5;241m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_dtype_via_object_chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1021\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_filelike\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimaginary_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimaginary_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiplines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilelike\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilelike\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbyte_converters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbyte_converters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;66;03m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;66;03m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;66;03m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;66;03m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filelike:\n","\u001b[0;31mValueError\u001b[0m: could not convert string '76.2012196342366 6.6705621039015695' to float64 at row 0, column 1."],"ename":"ValueError","evalue":"could not convert string '76.2012196342366 6.6705621039015695' to float64 at row 0, column 1.","output_type":"error"}],"id":"cdf279c7-b782-4a39-8b67-3ecd544888f0"},{"cell_type":"code","source":"","metadata":{"tags":[]},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'/home/jovyan/Muon_decay_GAN/GAN/code'"},"metadata":{}}],"id":"83408563-604b-41b8-b569-4e98e8f5eb0c"},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport csv\nimport os\n\n# Check device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Generator Network\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Linear(2, 16),\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 16),\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 2),\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n# Discriminator Network\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Linear(2, 16),\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 16),\n            nn.LeakyReLU(0.2),\n            nn.Linear(16, 1),\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\n# Initialize models and move to device\ngenerator = Generator().to(device)\ndiscriminator = Discriminator().to(device)\n\n# Loss and optimizers\ncriterion = nn.BCEWithLogitsLoss()\ngen_optimizer = optim.RMSprop(generator.parameters(), lr=0.001)\ndisc_optimizer = optim.RMSprop(discriminator.parameters(), lr=0.001)\n\n# Noise sampling\ndef sample_Z(m, n):\n    return torch.Tensor(np.random.uniform(-1., 1., size=[m, n])).to(device)\n\n# Load your nuclear flux data (adjust path as needed)\ndef load_data(file_path):\n    data = np.loadtxt(file_path, usecols=[0,1])\n    return torch.Tensor(data).to(device)\n\nreal_data_file = '/home/jovyan/Muon_decay_GAN/GAN/code/m_hist.txt'\nx_plot = load_data(real_data_file)\n\n# Training params\nbatch_size = min(256, len(x_plot))\nnd_steps = 10\nng_steps = 10\nnum_iterations = 100001\n\n# Setup directories\nlog_dir = '../plots'\ncheckpoint_dir = os.path.join(log_dir, 'checkpoint')\nplot_dir = os.path.join(log_dir, 'iterations')\nos.makedirs(checkpoint_dir, exist_ok=True)\nos.makedirs(plot_dir, exist_ok=True)\n\nloss_log_file = os.path.join(log_dir, 'loss_log.csv')\n\n# Checkpoint saving\ndef save_checkpoint(iteration):\n    path = os.path.join(checkpoint_dir, f'checkpoint_{iteration}.tar')\n    torch.save({\n        'iteration': iteration,\n        'generator_state_dict': generator.state_dict(),\n        'discriminator_state_dict': discriminator.state_dict(),\n        'gen_optimizer_state_dict': gen_optimizer.state_dict(),\n        'disc_optimizer_state_dict': disc_optimizer.state_dict(),\n    }, path)\n    print(f\"[Checkpoint] Saved at iteration {iteration}\")\n\n# Checkpoint loading\ndef load_checkpoint():\n    files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.tar')]\n    if not files:\n        print(\"[Checkpoint] No checkpoint found. Starting fresh.\")\n        return 0\n    latest = max(files, key=lambda f: int(f.split('_')[1].split('.')[0]))\n    path = os.path.join(checkpoint_dir, latest)\n    checkpoint = torch.load(path)\n    generator.load_state_dict(checkpoint['generator_state_dict'])\n    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n    gen_optimizer.load_state_dict(checkpoint['gen_optimizer_state_dict'])\n    disc_optimizer.load_state_dict(checkpoint['disc_optimizer_state_dict'])\n    print(f\"[Checkpoint] Loaded from iteration {checkpoint['iteration']}\")\n    return checkpoint['iteration']\n\n# Load checkpoint if exists\nstart_iteration = load_checkpoint()\n\n# Initialize CSV log file if fresh start\nif start_iteration == 0:\n    with open(loss_log_file, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Iteration', 'Discriminator Loss', 'Generator Loss'])\n\n# Training loop\nfor i in range(start_iteration, num_iterations):\n    for _ in range(nd_steps):\n        indices = np.random.choice(len(x_plot), batch_size, replace=False)\n        real_batch = x_plot[indices]\n        noise_batch = sample_Z(batch_size, 2)\n\n        real_labels = torch.ones(batch_size, 1).to(device)\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n\n        # Discriminator real loss\n        real_out = discriminator(real_batch)\n        real_loss = criterion(real_out, real_labels)\n\n        # Discriminator fake loss\n        fake_data = generator(noise_batch).detach()\n        fake_out = discriminator(fake_data)\n        fake_loss = criterion(fake_out, fake_labels)\n\n        disc_loss = real_loss + fake_loss\n\n        disc_optimizer.zero_grad()\n        disc_loss.backward()\n        disc_optimizer.step()\n\n    for _ in range(ng_steps):\n        noise_batch = sample_Z(batch_size, 2)\n        fake_data = generator(noise_batch)\n        fake_out = discriminator(fake_data)\n        gen_loss = criterion(fake_out, real_labels)\n\n        gen_optimizer.zero_grad()\n        gen_loss.backward()\n        gen_optimizer.step()\n\n    # Log losses\n    with open(loss_log_file, 'a', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow([i, disc_loss.item(), gen_loss.item()])\n\n    if i % 100 == 0 or i == start_iteration:\n        print(f\"Iteration {i}: Disc Loss={disc_loss.item():.4f}, Gen Loss={gen_loss.item():.4f}\")\n\n    # Save checkpoint every 1000 iterations\n    if i % 1000 == 0 and i != start_iteration:\n        save_checkpoint(i)\n\n    # Save plot every 1000 iterations\n    if i % 1000 == 0:\n        with torch.no_grad():\n            generated_samples = generator(sample_Z(batch_size, 2)).cpu().numpy()\n            real_samples = x_plot.cpu().numpy()\n\n        plt.figure(figsize=(6,6))\n        plt.scatter(real_samples[:, 0], real_samples[:, 1], color='blue', alpha=0.6, label='Real Data')\n        plt.scatter(generated_samples[:, 0], generated_samples[:, 1], color='orange', alpha=0.6, label='Generated Data')\n        plt.legend()\n        plt.title(f\"GAN Flux Samples at Iteration {i}\")\n        plt.xlabel('X')\n        plt.ylabel('Y')\n        plt.grid(True)\n        plt.tight_layout()\n        plt.savefig(os.path.join(plot_dir, f'iteration_{i}.png'), dpi=300)\n        plt.close()\n","metadata":{"tags":[],"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[Checkpoint] Loaded from iteration 95000\nIteration 95000: Disc Loss=1.2610, Gen Loss=0.5782\nIteration 95100: Disc Loss=1.2679, Gen Loss=0.6016\nIteration 95200: Disc Loss=1.2953, Gen Loss=0.6222\nIteration 95300: Disc Loss=1.3613, Gen Loss=0.6810\nIteration 95400: Disc Loss=1.3550, Gen Loss=0.6854\nIteration 95500: Disc Loss=1.3630, Gen Loss=0.6710\nIteration 95600: Disc Loss=1.1213, Gen Loss=0.5490\nIteration 95700: Disc Loss=1.3719, Gen Loss=0.6625\nIteration 95800: Disc Loss=1.3774, Gen Loss=0.6935\nIteration 95900: Disc Loss=1.3682, Gen Loss=0.7225\nIteration 96000: Disc Loss=0.8097, Gen Loss=0.2986\n[Checkpoint] Saved at iteration 96000\nIteration 96100: Disc Loss=1.3612, Gen Loss=0.5523\nIteration 96200: Disc Loss=1.3248, Gen Loss=0.6518\nIteration 96300: Disc Loss=0.8137, Gen Loss=0.4697\nIteration 96400: Disc Loss=1.3067, Gen Loss=0.6185\nIteration 96500: Disc Loss=1.3423, Gen Loss=0.5969\nIteration 96600: Disc Loss=0.9300, Gen Loss=0.3383\nIteration 96700: Disc Loss=1.3819, Gen Loss=0.3608\nIteration 96800: Disc Loss=1.2087, Gen Loss=0.5633\nIteration 96900: Disc Loss=1.3773, Gen Loss=0.6817\nIteration 97000: Disc Loss=1.3405, Gen Loss=0.6585\n[Checkpoint] Saved at iteration 97000\nIteration 97100: Disc Loss=1.2610, Gen Loss=0.5999\nIteration 97200: Disc Loss=1.2931, Gen Loss=0.6137\nIteration 97300: Disc Loss=1.3276, Gen Loss=0.6559\nIteration 97400: Disc Loss=1.3702, Gen Loss=0.6412\nIteration 97500: Disc Loss=1.3272, Gen Loss=0.6900\nIteration 97600: Disc Loss=1.1569, Gen Loss=0.5769\nIteration 97700: Disc Loss=1.5605, Gen Loss=0.3559\nIteration 97800: Disc Loss=1.3321, Gen Loss=0.6585\nIteration 97900: Disc Loss=1.0876, Gen Loss=0.5070\nIteration 98000: Disc Loss=1.3000, Gen Loss=0.6261\n[Checkpoint] Saved at iteration 98000\nIteration 98100: Disc Loss=1.2801, Gen Loss=0.6116\nIteration 98200: Disc Loss=0.9320, Gen Loss=0.4440\nIteration 98300: Disc Loss=1.0838, Gen Loss=0.5181\nIteration 98400: Disc Loss=1.2796, Gen Loss=0.5858\nIteration 98500: Disc Loss=1.3243, Gen Loss=0.6330\nIteration 98600: Disc Loss=1.4407, Gen Loss=0.5938\nIteration 98700: Disc Loss=1.2636, Gen Loss=0.5790\nIteration 98800: Disc Loss=1.3701, Gen Loss=0.6093\nIteration 98900: Disc Loss=1.1942, Gen Loss=0.5003\nIteration 99000: Disc Loss=1.3276, Gen Loss=0.5374\n[Checkpoint] Saved at iteration 99000\nIteration 99100: Disc Loss=1.2095, Gen Loss=0.5168\nIteration 99200: Disc Loss=1.5172, Gen Loss=0.2816\nIteration 99300: Disc Loss=1.2994, Gen Loss=0.5550\nIteration 99400: Disc Loss=1.0605, Gen Loss=0.3623\nIteration 99500: Disc Loss=1.0402, Gen Loss=0.3925\nIteration 99600: Disc Loss=1.0729, Gen Loss=0.5124\nIteration 99700: Disc Loss=1.3076, Gen Loss=0.5914\nIteration 99800: Disc Loss=1.2977, Gen Loss=0.6426\nIteration 99900: Disc Loss=1.2604, Gen Loss=0.5380\nIteration 100000: Disc Loss=1.2515, Gen Loss=0.5419\n[Checkpoint] Saved at iteration 100000\n","output_type":"stream"}],"id":"f5df4d3f-2ab9-4b26-8896-c880917ca027"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"7a612ce4-9b5e-4b01-9616-5ba3e8a2de07"},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[],"id":"f2e71de9-31b1-4041-a800-60062937c987"}]}